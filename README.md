# GRPO implementation from scratch
A naive implementation of GRPO (Group Relative Policy Optimization). 
This implementation is only meant to understand the algorithm. 
It is designed to run in a single GPU (I used a 4090). 
You can set a relative normal batch size and number of generation, because I use micro batch forward and backward. 
This feature enable you to train model more stably. 

è¿™ä¸ªå®ç°åªæ˜¯ä¸ºäº†ç†è§£ç®—æ³•ã€‚
å®ƒè®¾è®¡ç”¨äºåœ¨å• GPUï¼ˆæˆ‘ä½¿ç”¨çš„æ˜¯ 4090ï¼‰ä¸Šè¿è¡Œã€‚
ä½ å¯ä»¥è®¾ç½®ç›¸å¯¹æ­£å¸¸çš„æ‰¹æ¬¡å¤§å°å’Œç”Ÿæˆæ¬¡æ•°ï¼Œå› ä¸ºæˆ‘ä½¿ç”¨çš„æ˜¯å‘å‰å’Œå‘åçš„å¾®æ‰¹æ¬¡ã€‚
è¿™ä¸€åŠŸèƒ½å¯ä»¥è®©ä½ æ›´ç¨³å®šåœ°è®­ç»ƒæ¨¡å‹ã€‚

## How to run this repo

### Setup

```bash
conda create -n grpo python=3.10 -y
conda activate grpo
pip install torch
pip install transformers datasets wandb
```

### Run code

```bash
cd src
bash run_train.sh
```

This will create a log file as `run_train.log`. 

I only use partial data of `gsm8k` dataset (1024 QAs), and training on 4090 takes 2h approximately. 

### Run GRPO trainer by huggingface

```bash
cd src_grpo_trainer
bash train.sh
```

## ç›¸å…³æ•™ç¨‹

- GRPO & DAPO è®ºæ–‡è§£è¯»ï¼š https://shar-pen.github.io/2025/07/31/Reinforcement_learning/GRPO&DAPO_paper/
- GRPO å®ç°è®²è§£ï¼š https://shar-pen.github.io/2025/08/02/Reinforcement_learning/GRPO_implementation_from_scratch/
- GRPO-trainer-HF é•¿åº¦å¥–åŠ±çš„æ–‡æœ¬å‹ç¼©ä»»åŠ¡ï¼š https://shar-pen.github.io/2025/07/31/Reinforcement_learning/GRPO_length_reward/
- GRPO trainer è®­ç»ƒæ¨ç†æ¨¡å‹ï¼š https://shar-pen.github.io/2025/08/02/Reinforcement_learning/GRPO_trainer_reasoning_model/
- Direct Preference Optimization (DPO)ï¼š https://shar-pen.github.io/2025/06/24/Reinforcement_learning/DPO/
- DPO trainer - by trlï¼š https://shar-pen.github.io/2025/07/16/Reinforcement_learning/DPO_trainer_exp/



## GRPO explained

GRPO èµ·æºäº [DeepSeekMath](https://arxiv.org/abs/2402.03300), æ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚GRPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**ç»„å†…ç›¸å¯¹å¥–åŠ±**æ¥ä¼°è®¡åŸºçº¿ï¼ˆbaselineï¼‰ï¼Œä»è€Œé¿å…ä½¿ç”¨é¢å¤–çš„ä»·å€¼å‡½æ•°æ¨¡å‹ï¼ˆcritic modelï¼‰ä¸ PPO ç›¸æ¯”ï¼Œæ˜¾ç€å‡å°‘äº†è®­ç»ƒèµ„æº (PS: ä»–ä¸éœ€è¦ actor-criticç½‘è·¯ï¼Œä¸¤ä¸ªä¸è®­ç»ƒæ¨¡å‹ç›¸å½“çš„æ¨¡å‹)ã€‚ä¼ ç»Ÿçš„PPOç®—æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªä»·å€¼å‡½æ•°æ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ï¼ˆadvantage functionï¼‰ï¼Œè€ŒGRPOé€šè¿‡ä»åŒä¸€é—®é¢˜çš„å¤šä¸ªè¾“å‡ºä¸­è®¡ç®—å¹³å‡å¥–åŠ±æ¥æ›¿ä»£è¿™ä¸€è¿‡ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

![image-20250711173702271](images/README/image-20250711173702271-1754141764854-1.png)

ä»å›¾ä¸Šå¯ä»¥çœ‹å‡ºï¼ŒGRPO ä¸PPO çš„ä¸»è¦åŒºåˆ«æœ‰ï¼š

- GRPO çœç•¥äº† value function model å’Œ reward model.
- GRPO reward è®¡ç®—ï¼Œæ”¹æˆäº†ä¸€ä¸ªé—®é¢˜ q ç”Ÿæˆå¤šä¸ªå›ç­” r, ç„¶ååŸºäº reward  functionæ‰“åˆ†ã€‚
- PPO ä¼˜åŠ¿å‡½æ•°è®¡ç®—æ—¶ï¼ŒKL æ˜¯åŒ…å«åœ¨GAEå†…éƒ¨çš„ã€‚ GRPO ç›´æ¥æŒªåˆ°äº†å¤–é¢ï¼ŒåŒæ—¶ä¿®æ”¹äº†è®¡ç®—æ–¹æ³•ã€‚



### PPO


$$
\mathcal{J}\_{\text{PPO}}(\theta) = \mathbb{E}\_{q \sim D,\ o \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \min \left( r(\theta) \hat{A}\_t,\ \text{clip} \left( r(\theta),\ 1 - \varepsilon,\ 1 + \varepsilon \right) \hat{A}\_t \right) \right]
$$

å…¶ä¸­ï¼š
$$
r(\theta) = \frac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}
$$

- $r_t(\theta)$ æ˜¯å½“å‰ç­–ç•¥ä¸æ—§ç­–ç•¥çš„æ¯”å€¼ï¼Œæ³¨æ„ r ä»£è¡¨ ratio è€Œä¸æ˜¯ rewardï¼›
- $\hat{A}\_t$â€‹ æ˜¯ä¼°è®¡å‡ºçš„ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰ï¼Œè¡¨ç¤ºå½“å‰åŠ¨ä½œç›¸å¯¹å¹³å‡ç­–ç•¥çš„å¥½åï¼›
- $\epsilon$ æ˜¯è¶…å‚æ•°ï¼Œæ§åˆ¶å…è®¸çš„ç­–ç•¥å˜åŠ¨èŒƒå›´ï¼ˆå¦‚ 0.2ï¼‰ï¼›
- `clip` æ“ä½œå°† $r_t$ é™åˆ¶åœ¨ $[1-\epsilon, 1+\epsilon]$ï¼›
- å– `min` æ˜¯ä¸ºäº†åœ¨è¶…è¿‡ clip åŒºé—´æ—¶ä½¿ç”¨â€œæƒ©ç½šå€¼â€ï¼Œé˜²æ­¢è¿‡åº¦ä¼˜åŒ–ã€‚

---

clip å¯¹æ¢¯åº¦æ›´æ–°çš„å½±å“æ˜¯

| Advantage ç¬¦å· $\hat{A}\_t$ | $r_t$ åŒºé—´                                | åŸå§‹é¡¹ $r_t \hat{A}\_t$ | clipé¡¹ $\text{clip}(r_t, 1-\epsilon, 1+\epsilon) \hat{A}\_t$ | å®é™…å€¼ = min(...)         | æ˜¯å¦å‘ç”Ÿclipé™åˆ¶ |
| -------------------------- | ----------------------------------------- | ---------------------- | ----------------------------------------------------------- | ------------------------- | ---------------- |
| $\hat{A}\_t > 0$            | $r_t < 1 - \epsilon$                      | $r_t \hat{A}\_t$        | $(1 - \epsilon)\hat{A}\_t$                                   | $r_t \hat{A}\_t$           | N                |
| $\hat{A}\_t > 0$            | $1 - \epsilon \leq r_t \leq 1 + \epsilon$ | $r_t \hat{A}\_t$        | $r_t \hat{A}\_t$                                             | $r_t \hat{A}\_t$           | N                |
| $\hat{A}\_t > 0$            | $r_t > 1 + \epsilon$                      | $r_t \hat{A}\_t$        | $(1 + \epsilon)\hat{A}\_t$                                   | $(1 + \epsilon)\hat{A}\_t$ | Y                |
| $\hat{A}\_t < 0$            | $r_t < 1 - \epsilon$                      | $r_t \hat{A}\_t$        | $(1 - \epsilon)\hat{A}\_t$                                   | $(1 - \epsilon)\hat{A}\_t$ | Y                |
| $\hat{A}\_t < 0$            | $1 - \epsilon \leq r_t \leq 1 + \epsilon$ | $r_t \hat{A}\_t$        | $r_t \hat{A}\_t$                                             | $r_t \hat{A}\_t$           | N                |
| $\hat{A}\_t < 0$            | $r_t > 1 + \epsilon$                      | $r_t \hat{A}\_t$        | $(1 + \epsilon)\hat{A}\_t$                                   | $r_t \hat{A}\_t$           | N                |

**ç›®æ ‡æ˜¯æŠ‘åˆ¶ç­–ç•¥æ¯”ä¾‹å˜åŒ– $r_t$ å¤ªå¤§æˆ–å¤ªå°å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸æˆ–å´©å¡Œ**ã€‚

- å¦‚æœ Advantage æ˜¯æ­£çš„ï¼ˆåŠ¨ä½œå¥½ï¼‰ï¼Œå¸Œæœ›å¢åŠ æ¦‚ç‡ï¼Œä½† clip ä¼šé™åˆ¶å…¶æ¯”ä¾‹æœ€å¤šå¢åŠ åˆ° $1+\epsilon$ã€‚
- å¦‚æœ Advantage æ˜¯è´Ÿçš„ï¼ˆåŠ¨ä½œå·®ï¼‰ï¼Œå¸Œæœ›å‡å°‘æ¦‚ç‡ï¼Œä½† clip ä¼šé™åˆ¶å…¶æ¯”ä¾‹æœ€å¤šå‡å°‘åˆ° $1-\epsilon$ã€‚

æ³¨æ„è¿™ç‚¹ï¼ŒDAPO ä¼šåœ¨è¿™é‡Œæ”¹è¿›ã€‚

---

è¿™æ˜¯å…¸å‹çš„ **PPO (Proximal Policy Optimization)** æŸå¤±å‡½æ•°ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å¦‚RLHFæˆ–GRPOä¸­ã€‚å…¶ä¸­ $\pi_{\theta}$ and $\pi_{\theta_{\text{old}}}$ åˆ†åˆ«æ˜¯å½“å‰çš„ç­–ç•¥æ¨¡å‹å’Œæ—§çš„ç­–ç•¥æ¨¡å‹ï¼Œ $\text{clip}$ æ“ä½œç”¨äºé™åˆ¶ç­–ç•¥æ¯”ç‡çš„å˜åŒ–ï¼Œä»è€Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ã€‚$A_t$ æ˜¯ä¼˜åŠ¿ï¼Œé€šè¿‡ Generalized Advantage Estimation (GAE) ï¼ŒåŸºäºä¸€ç»„å¥–åŠ±å’Œä¸€ä¸ªå¥–åŠ±å‡½æ•°è®¡ç®—å‡ºã€‚åœ¨ PPO ä¸­ï¼Œä»·å€¼å‡½æ•°éœ€è¦ä¸ç­–ç•¥æ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œä¸ºäº†å‡è½»å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–ï¼Œæ ‡å‡†æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªtoken çš„å¥–åŠ±ä¸­ä»å‚è€ƒæ¨¡å‹æ·»åŠ æ¯ä¸ªtokençš„ KL æ•£åº¦æƒ©ç½šé¡¹ã€‚

### GRPO

#### objective

ä¸ PPO ç›¸æ¯”ï¼ŒGRPO æ¶ˆé™¤äº†ä»·å€¼å‡½æ•°ï¼Œå¹¶ä»¥ç¾¤ä½“ç›¸å¯¹æ–¹å¼ä¼°è®¡ä¼˜åŠ¿ã€‚å¯¹äºç‰¹å®šçš„é—®é¢˜-ç­”æ¡ˆå¯¹ (q, a)ï¼Œè¡Œä¸ºç­–ç•¥ $\pi_{\text{old}}$ é‡‡æ ·ä¸€ç»„ G ä¸ªç‹¬ç«‹å“åº” $\{o_i\}\_{i=1}^G$ã€‚ç„¶åï¼Œé€šè¿‡å½’ä¸€åŒ–ç¾¤ä½“çº§å¥–åŠ± $\{R_i\}\_{i=1}^G$ è®¡ç®—ç¬¬ $i$ ä¸ªå“åº”çš„ä¼˜åŠ¿ï¼š
$$
\hat{A}\_{i,t} = \frac{r_i - \text{mean}\left(\{R_i\}\_{i=1}^G\right)}{\text{std}\left(\{R_i\}\_{i=1}^G\right)}.
$$
ä¸ PPO ç±»ä¼¼ï¼ŒGRPOåœ¨åŸå§‹ç›®æ ‡çš„åŸºç¡€ä¸ŠåŠ ä¸Šäº† KL æ•£åº¦æƒ©ç½šé¡¹ ï¼ˆ**é™åˆ¶å½“å‰ç­–ç•¥ä¸å‚è€ƒç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œä¸è®©ç­–ç•¥å˜åŒ–å¤ªæ¿€è¿›ã€‚**PPOæ˜¯åœ¨ reward é‡ŒåŠ å…¥KL é‡å¤é¡¹ï¼ŒGRPO reward è®¡ç®—ä¸åŒï¼Œå› æ­¤ç›´æ¥é¢å¤–åŠ å…¥ KL é¡¹ï¼‰

ä»å›¾ç‰‡ä¸­æå–çš„å…¬å¼å¦‚ä¸‹ï¼š
$$
\mathcal{J}\_{\text{GRPO}}(\theta) = \mathbb{E}\_{(q,a) \sim \mathcal{D}, \{o_i\}\_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)} \Bigg[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \Big( \min \big( r_{i,t}(\theta) \hat{A}\_{i,t}, \, \text{clip}( r_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon ) \hat{A}\_{i,t} \big) - \beta D_{\mathrm{KL}}(\pi_\theta \| \pi_{\text{ref}}) \Big) \Bigg].
$$

- $(q,a) \sim \mathcal{D}$ï¼šé—®é¢˜-ç­”æ¡ˆå¯¹æ¥è‡ªæ•°æ®é›†ã€‚
- $\{o_i\}\_{i=1}^G$ï¼šä»æ—§ç­–ç•¥ $\pi_{\text{old}}$ ç”Ÿæˆçš„ $G$ ä¸ªå€™é€‰è¾“å‡ºï¼ˆrolloutsï¼‰ã€‚
- $|o_i|$ï¼šç¬¬ $i$ ä¸ªè¾“å‡ºçš„ token é•¿åº¦ã€‚
- $r(\theta) = \pi_{\theta}(o_t | q, o_{<t}) / \pi_{\theta_{\text{old}}}(o_t | q, o_{<t})$ 

$$
\mathbb{D}\_{\mathrm{KL}} \left[ \pi_\theta \,\|\, \pi_{\text{ref}} \right] = \frac{ \pi_{\text{ref}}(o_{i,t} \mid q, o_{i,<t}) }{ \pi_\theta(o_{i,t} \mid q, o_{i,<t}) } - \log \frac{ \pi_{\text{ref}}(o_{i,t} \mid q, o_{i,<t}) }{ \pi_\theta(o_{i,t} \mid q, o_{i,<t}) } - 1
$$

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGRPO åœ¨ **sample-level æ ·æœ¬çº§åˆ«è®¡ç®—ç›®æ ‡**ã€‚å…·ä½“æ¥è¯´ï¼ŒGRPO é¦–å…ˆ sample å†… losså…ˆå¹³å‡åŒ–ï¼Œå† sample é—´ loss å¹³å‡åŒ–ã€‚è¿™ä¸€ç‚¹åœ¨ DAPO é‡Œä¼šæ”¹å˜ã€‚

---

#### ç®—æ³•æµç¨‹

![image-20250722144246828](images/README/image-20250722144246828-1754141764854-2.png)

å¯¹äºæ¯ä¸€æ‰¹æ•°æ®

- å…ˆæ›´æ–° $\pi_{old}=\pi_\theta$ 
- ç”¨ $\pi_{old}$ ä¸ºæ¯ä¸ªé—®é¢˜äº§ç”Ÿ $G$ ä¸ª rollout
- ç”¨ reward model/function è®¡ç®—æ¯æ¡ rollout $o_i$ çš„å¥–åŠ± $r_i$
- ç”¨ç»„å†…ä¼˜åŒ–ä¼°è®¡(å½’ä¸€åŒ–)çš„æ–¹å¼è®¡ç®— $A_i$ (æˆ–è€… $A_i^t$ï¼ŒGRPO reward/adavantage éƒ½æ˜¯ sample-levelçš„ï¼ŒåŒä¸€ sample å†…æ¯ä¸ªtokenéƒ½ä¸€æ ·)
- è¿­ä»£ç”¨ GRPO loss æ›´æ–° $\pi_\theta$

ä»¥ä¸‹æ˜¯ DeepseekMath ä¸­æåŠçš„è¶…å‚:

| è¶…å‚æ•°åç§°          | è¯´æ˜                                                         | è®ºæ–‡ä¸­è®¾ç½®å€¼ |
| ------------------- | ------------------------------------------------------------ | ------------ |
| ğœ€ï¼ˆepsilonï¼‰        | clip å‚æ•°ï¼Œæ§åˆ¶æ¦‚ç‡æ¯”çš„ä¸Šä¸‹ç•Œä»¥é˜²æ­¢è¿‡å¤§æ›´æ–°                  | 0.2          |
| ğ›½ï¼ˆbetaï¼‰           | KL æ­£åˆ™é¡¹ç³»æ•°ï¼Œç”¨äºé˜²æ­¢ policy åç¦» reference è¿‡è¿œ           | 0.04         |
| ğœ‡ï¼ˆmuï¼‰             | æ¯æ¬¡ rollout åå¯¹è¯¥ batch æ‰§è¡Œçš„ GRPO è®­ç»ƒè¿­ä»£æ¬¡æ•°           | 1            |
| Gï¼ˆgroup sizeï¼‰     | æ¯ä¸ªé—®é¢˜é‡‡æ ·çš„å›ç­”æ•°é‡ï¼Œç”¨äºè®¡ç®— group å¹³å‡å¥–åŠ±ä½œä¸º baseline | 64           |
| Max Length          | æ¯ä¸ªå›ç­”çš„æœ€å¤§ token é•¿åº¦                                    | 1024         |
| Training Batch Size | æ¯æ¬¡è®­ç»ƒçš„ batch å¤§å°ï¼ˆæ€»ç”Ÿæˆæ ·æœ¬æ•°ï¼‰                        | 1024         |
| Policy LR           | Policy æ¨¡å‹çš„å­¦ä¹ ç‡                                          | 1e-6         |
| Reward LR           | å¥–åŠ±æ¨¡å‹çš„å­¦ä¹ ç‡                                             | 2e-5         |

---

#### reward

reward åŒ…æ‹¬ rule-based reward å’Œ reward modelï¼Œå‰è€…å®Œå…¨åŸºäºè§„åˆ™ï¼Œåè€…åŸºäºå¤§æ¨¡å‹ä½œä¸ºè¯„ä¼°æ¨¡å‹ï¼Œæ¨¡æ‹Ÿäººç±»åå¥½ã€‚DeepseekMath page 20 äº¤ä»£äº†å¥–åŠ±çš„è®¾ç½®ï¼Œ

> The algorithm processes the reward signal to the gradient coefficient to update the model parameter. 
>
> **We divide the reward function as â€˜Ruleâ€™ and â€˜Modelâ€™ in our experiments.** 
>
> - Rule refers to judging the quality of a response based on the correctness of the answer
> - Model denotes that we train a reward model to score each response. 
>
> The training data of the reward model is based on the rule judgment. 

---

Deepseek r1-zeroä½¿ç”¨äº†è§„åˆ™å¥–åŠ± (åŒæ—¶å› ä¸ºåªæœ‰å¯é€šè¿‡è§„åˆ™è¯„ä¼°çš„æ•°æ®):

- Accuracy rewards: The accuracy reward model evaluates whether the response is correct. ç»“æœçš„å‡†ç¡®æ€§å¥–åŠ±
- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between â€˜<think>â€™ and â€˜</think>â€™ tags. æ ¼å¼å¥–åŠ±ï¼Œä¸»è¦æ˜¯ tagã€‚

ä»–ä»¬è¯•è¿‡è¿‡ç¨‹å¥–åŠ±ï¼Œä½†æ•ˆæœä¸è¡Œï¼Œå®¹æ˜“ reward hackingï¼Œä¸”ç»´æŠ¤ reward model éœ€è¦é¢å¤–è®¡ç®—èµ„æºï¼Œå› æ­¤æ”¾å¼ƒäº†ã€‚

---

Deepseek r1 æ‰©å……äº†é¢å¤–æ•°æ®ï¼Œå…¶ä¸­ä½¿ç”¨ç”Ÿæˆå¼çš„ reward model æ¥è¯„ä¼° (åŒæ—¶ç»™ ground-truth å’Œ prediction)ã€‚Deepseek R1 page 11 è¯´æ˜äº† reward model æ˜¯åŸºäº deepseek v3 å’Œåå¥½æ•°æ®è®­ç»ƒå‡ºæ¥çš„ï¼Œè¯„ä¼°ç»´åº¦ä¸º helpfulness å’Œ harmlessnessã€‚

> Specifically, we train the model using a combination of reward signals and diverse prompt distributions. 
>
> - For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. 
> - For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios.
>
> We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. 
>
> - For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. 
> - For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. 
>
> Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.

åŒæ—¶ï¼Œdeepseek-r1 zero è®­ç»ƒæ—¶å‡ºç°è¯­è¨€æ··ä¹±çš„æƒ…å†µï¼Œä¸ºäº†è§£å†³è¿™ç‚¹ï¼Œdeepseek-r1 åŠ å…¥äº†è¯­è¨€ä¸€è‡´æ€§ rewardï¼ŒåŸºäº CoT ä¸­ç›®æ ‡è¯­è¨€çš„æ¯”ä¾‹è®¡ç®—ã€‚è™½ç„¶è¿™å›å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¼šå°‘é‡é™ä½ï¼Œä½†å’Œäººç±»åå¥½æ›´å¯¹é½ã€‚

## 

## GRPO implementation explained

### æ•°æ®å‡†å¤‡ src/prepare_data.py

å°†æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆå‡†å¤‡å¥½ï¼Œåœ¨  `openai/gsm8k` æ•°æ®é›†ä¸­ answer å­—æ®µçš„ç­”æ¡ˆæ˜¯é•¿ç­”æ¡ˆ + ### åçš„çŸ­ç­”æ¡ˆã€‚åœ¨åˆ¤æ–­æ˜¯å¦æ¨¡å‹ç­”å¯¹é—®é¢˜æ—¶ï¼Œä¸ä¼šä¸­é—´è¿‡ç¨‹è¿›è¡Œè¯„ä¼°ï¼Œè€Œæ˜¯å¯¹ç»“æœè¯„ä¼°ï¼Œå› æ­¤å®é™…åªéœ€è¦å°†çŸ­ç­”æ¡ˆæå–ã€‚

ä»¥ä¸‹æ˜¯ `openai/gsm8k` çš„æ•°æ®å¯¹ç¤ºä¾‹:

```markdown
Question:
Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?

Answer:
Natalia sold 48/2 = <<48/2=24>>24 clips in May.
Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.
#### 72
```

ä»¥ä¸‹æ˜¯ä»£ç ä¸­çš„å¤„ç†ä»£ç ï¼Œæˆ‘ä»¬å°†æ ¼å¼è¦æ±‚ä½œä¸º system promptï¼Œä¸é—®é¢˜æ„æˆå¯¹è¯ã€‚éƒ¨åˆ†å®ç°é‡Œä¼šåŠ å…¥one shot QA ç¤ºä¾‹ã€‚

```python
efault_system_prompt = """A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think>\n reasoning process here \n</think>\n<answer>\n answer here \n</answer>.
"""

def extract_final_answer(text):
	if "####" not in text:
		return None
	return text.split("####")[1].strip()


def make_conversation(example, system_prompt=None):
	prompt = []

	if system_prompt is not None:
		prompt.append({"role": "system", "content": system_prompt})
	
	prompt.append({"role": "user", "content": example['question']})

	return {"prompt": prompt, "solution": extract_final_answer(example['answer'])}

dataset = load_dataset('openai/gsm8k', 'main', split='train')

dataset_formatted = dataset.map(
    partial(
        make_conversation, 
        system_prompt=system_prompt,
    ),
)
dataset_formatted = dataset_formatted.map(
    partial(add_len, tokenizer=tokenizer),
)
```

ç”±äºæœ¬é¡¹ç›®æ˜¯æœ€ä½çš„å®ç°ï¼Œæˆ‘å¯¹é—®é¢˜å’Œç­”æ¡ˆé•¿åº¦è¿›è¡Œäº†é™åˆ¶ï¼Œå¸Œæœ›æ¨¡å‹ä¸ä¼šé‡åˆ°å¤ªå¤æ‚çš„é—®é¢˜è€Œäº§ç”Ÿå¾ˆé•¿çš„ responseã€‚

```python
def add_len(example, tokenizer):
    # è®¡ç®— token æ•°ï¼›å»æ‰ special tokens ä¿æŒä¸€è‡´æ€§
    prompt_ids  = tokenizer.apply_chat_template(example["prompt"], tokenize=True, add_generation_prompt=True)
    answer_ids  = tokenizer.encode(example["answer"],  add_special_tokens=False)
    example["prompt_len"]  = len(prompt_ids)
    example["answer_len"]  = len(answer_ids)
    return example

dataset_formatted = dataset_formatted.filter(
    lambda x: x["prompt_len"] <= 300 and x["answer_len"] <= 200,
)
dataset_formatted = dataset_formatted.select(range(1024))

```



### æ¨¡å‹å¤„ç† src/model_utils.py

å¾ˆçŸ­ï¼Œä»…æœ‰ä¸¤ä¸ªå‡½æ•°

- ä¼˜åŒ–æ¨¡å‹è®¾ç½®: ä¿®æ”¹è®¾ç½®ï¼Œä½¿æ˜¾å­˜å ç”¨é™ä½
- å†»ç»“æ¨¡å‹: å‚è€ƒæ¨¡å‹éœ€è¦å†»ç»“ï¼Œä»¥ç”¨äºè®¡ç®—KLæ•£åº¦

```python
def optimize_model_settings(model):
	model.config.use_cache = False
	model.gradient_checkpointing_enable()


def freeze_model(model):
	model.eval()
	for param in model.parameters():
		param.requires_grad = False
```



### grpoç›¸å…³util src/grpo_utils.py

æœ‰3ä¸ªå°è£…çš„å‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°éƒ½æœ‰æ¯ä¸ªæ­¥éª¤çš„æ³¨é‡Šã€‚

- log prob çš„è®¡ç®—å‡½æ•°
- completion çš„æ©ç 
- rollout ç”Ÿæˆ
- grpo loss è®¡ç®—

log prob çš„è®¡ç®—å°±æ˜¯æ­£å¸¸ label shift åï¼Œè·å–å¯¹åº” label çš„ log probï¼Œæ³¨æ„ä¸éœ€è¦ sumï¼Œå› ä¸ºåé¢è¿˜éœ€è¦åŠ ä¸Š KL æ•£åº¦ã€‚transformers æœ‰è¿™ä¸ªå‡½æ•°ï¼Œä½†æˆ‘è¿˜æ˜¯é‡å†™äº†ã€‚

```python
def get_per_token_log_probs(
	model: AutoModelForCausalLM,
	input_ids: torch.Tensor,
	attention_mask: torch.Tensor,
):
	"""
	è®¡ç®—æ¯ä¸ª token çš„ log-probability
	"""
	# label shift
	target_ids = input_ids[:, 1:]
	logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]
	# è®¡ç®— per token çš„ log-probability
	# æ ¹æ® `input_ids` è¿™ä¸ªç´¢å¼•(vocab id)ï¼Œä» `log_probs` é‡Œå–å‡ºå¯¹åº”ä½ç½®çš„ log-probability
	log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
	per_token_log_probs = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)

	return per_token_log_probs
```

completion çš„æ©ç ï¼Œæ ¹æ® eos_token_id æ¥è·å– completion çš„ä½ç½®ã€‚è¿™ä¸ªå‡½æ•°å‚è€ƒäº† grpo trainer ä¸­åŒåå‡½æ•°çš„å®ç°ã€‚

```python
def create_completion_mask(completion_ids, eos_token_id):
	"""
	æ ¹æ® completion_ids åˆ›å»º completion_maskï¼Œå°† eos_token_id ä¹‹åç½®ä¸º false
	"""
	#  ç”¨ mask æ’é™¤æ‰ eos token ä¹‹åçš„éƒ¨åˆ†ï¼Œä¿ç•™ prompt å’Œ completion çš„æœ‰æ•ˆéƒ¨åˆ†ï¼Œè®¡ç®— completion_mask çš„ç›®çš„æ˜¯è®¡ç®— completion å¯¹åº”çš„ log prob
	is_eos = completion_ids == eos_token_id
	eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)
	# æ£€æŸ¥æ¯ä¸€è¡Œæ˜¯å¦æœ‰ eos token
	mask_exists = is_eos.any(dim=1) 
	# å°†æœ‰ eos çš„è¡Œçš„ eos_idx è®¾ç½®ä¸ºå¯¹åº”çš„ eos token çš„ä½ç½®ï¼Œå…¶ä»–è¡Œä¿æŒ eos_idx.size(1) çš„å€¼ï¼Œ å³æœ€å¤§å€¼	
	eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]  
	# æ¯è¡Œå°±æ˜¯ [0, 1, 2, ..., max_completion_length-1] çš„åºåˆ—
	sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)
	# ç”Ÿæˆçš„ completion_maskï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆéƒ¨åˆ†ï¼Œ0 è¡¨ç¤ºæ— æ•ˆéƒ¨åˆ† 
	completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).to(torch.int64)
	return completion_mask
```

rollout ç”Ÿæˆï¼Œè™½ç„¶ transformers çš„ model.generate å‡½æ•°ä¸æ˜¯å¾ˆé«˜æ•ˆï¼Œgrpo trainer ä¸­ä¹Ÿé‡‡ç”¨ vllm æ¥ç”Ÿæˆ rolloutï¼Œä½†ä¸ºäº†ç®€æ˜“å‹æˆ‘è¿˜æ˜¯ç”¨äº† model.generateï¼Œå…¶å®è¿˜è·Ÿ vllm ä¸æ”¯æŒ jupyter notebook ç›¸å…³ã€‚

```python
@torch.no_grad()
def generate_rollouts(
	model: AutoModelForCausalLM, 
	tokenizer: AutoTokenizer, 
	prompts: list[list[dict[str, str]]] | list[str], # prompts maybe a list of list or list of str
	num_of_roullout:int=8,
	max_length: int = 1024,
	temperature: float = 1.0,
	top_p: float = 1.0,
	top_k: int = 50,
	):
	
	model.eval()
	device = model.device

	# 1. å‡†å¤‡ model inputs
	# 1.1 tokenize prompt
	if tokenizer.pad_token_id is None:
		tokenizer.pad_token_id = tokenizer.eos_token_id

	prompts = [
		maybe_apply_chat_template(a_prompt, tokenizer)
		for a_prompt in prompts
	]
	model_inputs = tokenizer(
		prompts,
		return_tensors="pt",
		padding=True,
		padding_side="left",
		return_attention_mask=True,
	).to(device)

	# 1.2 duplicate prompt num_rollouts times
	# input_ids å’Œ attention_mask éƒ½æ˜¯ bs(1) x sl çš„, éœ€è¦åœ¨ batch ç»´åº¦ä¸Šé‡å¤ num_rollouts æ¬¡
	model_inputs["input_ids"] = model_inputs["input_ids"].repeat_interleave(num_of_roullout, dim=0)
	model_inputs["attention_mask"] = model_inputs["attention_mask"].repeat_interleave(num_of_roullout, dim=0)
	# å– sl ç»´åº¦ä¸º prompt é•¿åº¦
	prompt_length = model_inputs["input_ids"].shape[1] 
	

	# 2. sample completions / rollouts
	generation_config = GenerationConfig(
		do_sample=True,
		top_p=top_p,
		top_k=top_k,
		temperature=temperature,
		max_length=max_length,
		pad_token_id=tokenizer.pad_token_id,
	)
	sequence_ids = model.generate(
		**model_inputs, 
		generation_config=generation_config
	)

	# 3. prepare return
	completions = tokenizer.batch_decode(
		sequence_ids[:, prompt_length:], skip_special_tokens=True
	)

	# completion mask æ˜¯æŒ‡ completion id å¯¹åº”çš„ mask
	# prompt éƒ¨åˆ†å…¨æ˜¯ 0ï¼Œ completion éƒ¨åˆ†éœ€è¦æ ¹æ® eos_token æ¥åŒºåˆ† completion çš„æœ‰æ•ˆéƒ¨åˆ†å’Œæ— æ•ˆéƒ¨åˆ†
	completion_mask = torch.zeros_like(sequence_ids, dtype=torch.int64)
	partial_completion_mask = create_completion_mask(sequence_ids[:, prompt_length:], tokenizer.eos_token_id)
	completion_mask[:, prompt_length:] = partial_completion_mask

	sequence_mask = torch.cat([model_inputs["attention_mask"], partial_completion_mask], dim=1)

	return sequence_ids, sequence_mask, completion_mask, completions
```

grpo loss è®¡ç®—ï¼Œæ¶‰åŠ log prob ratio çš„ clipï¼ŒKL æ•£åº¦çš„è®¡ç®—ï¼Œ completion mask æ©ç è®¡ç®— lossï¼Œå’Œå…ˆæ ·æœ¬å†…å¹³å‡åæ ·æœ¬é—´å¹³å‡ã€‚

```python
def get_grpo_loss(
	model: AutoModelForCausalLM,
	sequence_ids: torch.Tensor,
	sequence_mask: torch.Tensor,
	completion_mask: torch.Tensor,
	advantage_per_sample: torch.Tensor,
	prob_per_token_old: torch.Tensor,
	prob_per_token_reference: torch.Tensor,
	epsilon: float,
	beta: float = 0.04,
):
	
	# è®¡ç®— policy çš„ log prob
	prob_per_token_policy = get_per_token_log_probs(
		model,
		input_ids=sequence_ids,
		attention_mask=sequence_mask,
	)

	# è®¡ç®—æ¯ä¸ª token çš„ loss
	coef_1 = (prob_per_token_policy - prob_per_token_old).exp()
	coef_2 = torch.clamp(coef_1, 1 - epsilon, 1 + epsilon)
	loss_per_token_1 = coef_1 * advantage_per_sample.unsqueeze(1)
	loss_per_token_2 = coef_2 * advantage_per_sample.unsqueeze(1)
	loss_per_token = -torch.min(loss_per_token_1, loss_per_token_2)

	# per token çš„ KL æ•£åº¦
	kl_divergence_per_token = (prob_per_token_policy - prob_per_token_reference).exp() - (prob_per_token_policy - prob_per_token_reference) - 1
	loss_per_token += beta * kl_divergence_per_token

	# label shift completion_mask to match per_token_loss
	loss_per_completion = (loss_per_token * completion_mask[:, 1:]).sum(dim=1)
	length_per_completion = completion_mask[:, 1:].sum(dim=1).clamp(min=1)
	loss = (loss_per_completion / length_per_completion).mean()

	return loss

```

### è§„åˆ™å¥–åŠ±å‡½æ•° src/reward.py

åŒ…å«è‹¥å¹²è§„åˆ™å’Œ grpo çš„æœ€å†… advantage è®¡ç®—å‡½æ•°

æˆ‘ä½¿ç”¨äº† format å¥–åŠ±ã€xml tagå¥–åŠ±ã€å‡†ç¡®ç‡å¥–åŠ±

```python
def extract_answer(text):
	match = re.search(r'<answer>\n(.*?)\n</answer>', text, re.DOTALL)
	if match:
		return match.group(1).strip()
	return None


def format_reward(completion, **kwargs):
	"""
	æ£€æŸ¥é¢„æµ‹æ–‡æœ¬æ˜¯å¦ç¬¦åˆç‰¹å®šæ ¼å¼è¦æ±‚ã€‚e.g., <think>\n...\n</think>\n<answer>\n...\n</answer>
	kwargs å‚æ•°å¯ä»¥ç”¨äºä¼ é€’é¢å¤–çš„é…ç½®ï¼Œä½†åœ¨æ­¤å‡½æ•°ä¸­æœªä½¿ç”¨ã€‚
	"""
	pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
	if re.match(pattern, completion, re.DOTALL | re.MULTILINE):
		return 1.0
	else:
		return 0.0
	

def tag_count_reward(completion, **kwargs):
	"""
	æ£€æŸ¥æ–‡æœ¬ä¸­ <think> å’Œ <answer> æ ‡ç­¾çš„æ•°é‡ã€‚
	"""
	score = 0.0
	if completion.count("<think>\n") == 1:
		score += 0.25
	if completion.count("\n</think>\n") == 1:
		score += 0.25
	if completion.count("\n<answer>\n") == 1:
		score += 0.25
	if completion.count("\n</answer>") == 1:
		score += 0.25
	return score


def reasoning_steps_reward(completion, **kwargs):

	pattern = r"(Step \d+:|^\d+\.|\n-|\n\*|First,|Second,|Next,|Finally,)"
	matches = re.findall(pattern, completion)
	score = min(1.0, len(matches) / 3)  # å¥–åŠ± 3 æ¬¡ä»¥ä¸Š
	return score


def accuracy_reward(completion, solution, **kwargs):
	"""
	è®¡ç®—é¢„æµ‹æ–‡æœ¬ä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„å‡†ç¡®åº¦å¥–åŠ±ã€‚
	"""
	full_answer_content = extract_answer(completion)
	if full_answer_content is None:
		return 0.0

	gold_parsed = parse(solution)
	answer_parsed = parse(full_answer_content)

	try:
		score = float(verify(gold_parsed, answer_parsed))
	except Exception as e:
		print(f"verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}")
		return 0.0

	return score
	

def compute_grpo_reward(completions, solutions, reward_funcs, reward_weights=None):

	if reward_weights is None:
		reward_weights = [1.0/len(reward_funcs)] * len(reward_funcs)

	assert len(reward_weights) == len(reward_funcs), "reward_weight and reward_funcs must have the same length"

	rewards_per_sample_per_func = torch.zeros(len(completions), len(reward_funcs))

	for i, (a_completion, a_solution) in enumerate(zip(completions, solutions)):
		for j, reward_func in enumerate(reward_funcs):
			rewards_per_sample_per_func[i, j] = reward_func(a_completion, solution=a_solution)

	reward_weight_tensor = torch.tensor(reward_weights)
	reward_per_completion = (rewards_per_sample_per_func * reward_weight_tensor).sum(dim=1)

	# return avergaed score of different reward functions
	reward_per_reward_func = rewards_per_sample_per_func.mean(dim=0)

	return reward_per_completion, reward_per_reward_func
```

grpo ç»„å†… advantage è®¡ç®—ï¼Œæ³¨æ„ç»„æ˜¯æŒ‡åŒä¸€ä¸ªé—®é¢˜å†…éƒ¨è®¡ç®— mean å’Œ stdã€‚

```python
def compute_group_advantage(reward_per_sample: torch.Tensor, num_generations: int=None, eps: float = 1e-8, scale_rewards: bool = True):
	"""
	åŸºäº reward è®¡ç®— advantage
	"""
	if num_generations is None:
		num_generations = reward_per_sample.shape[0]

	# è®¡ç®—åŒä¸€ä¸ªpromptçš„å¤šæ¬¡ç”Ÿæˆçš„å¹³å‡å¥–åŠ±å’Œæ ‡å‡†å·®
	mean_grouped_rewards = reward_per_sample.view(-1, num_generations).mean(dim=1)
	std_grouped_rewards = reward_per_sample.view(-1, num_generations).std(dim=1)
	
	# å°† mean å’Œ std é‡å¤ num_generations æ¬¡ï¼Œä»¥ä¾¿ä¸ rewards çš„å½¢çŠ¶åŒ¹é…
	mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_generations, dim=0)
	std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_generations, dim=0)
	group_advantage = reward_per_sample - mean_grouped_rewards
	if scale_rewards:
		group_advantage /= (std_grouped_rewards + eps)

	return group_advantage

```



### grpo ä¸»å‡½æ•° src/main_minibatch.py

grpo çš„ä¸»è¦æµç¨‹åˆ†ä¸º:

- rollout ç”Ÿæˆ
- reward è®¡ç®—
- æ—§ policy model (å½“å‰ policy model)å’Œå‚è€ƒ model çš„ log prob è®¡ç®—
- grpo loss è®¡ç®—å¹¶åå‘ä¼ æ’­

#### rollout ç”Ÿæˆ

```python

prompts = [example['prompt'] for example in batch]
solutions = [example['solution'] for example in batch]

sequence_ids, sequence_mask, completion_mask, completions = generate_rollouts(
    model_policy, 
    tokenizer, 
    prompts, 
    num_of_roullout=n_roullout, 
    max_length=max_length, 
    temperature=1.0, 
    top_p=0.9, 
    top_k=50,
)
```



#### reward è®¡ç®—

```python
reward_funcs = [format_reward, tag_count_reward, accuracy_reward]
reward_weights = [0.5, 0.5, 1.0]

solutions = [s for s in solutions for _ in range(n_roullout)]

reward_per_completion, reward_per_reward_func = compute_grpo_reward(
    completions, 
    solutions, 
    reward_funcs,
    reward_weights,
)

group_advantage_per_sample = compute_group_advantage(
    reward_per_completion
).to(device)
```

reward weight æœ€å¥½å€¾å‘ accuracy_rewardï¼Œå…¶ä»–æ ¼å¼ç›¸å¯¹å¥½å­¦ä¹ åˆ°ã€‚

#### æ—§ policy model å’Œå‚è€ƒ model çš„ log prob è®¡ç®—

```python
with torch.no_grad():
    prob_per_token_old = []
    prob_per_token_reference = []

    for i in range(0, len(sequence_ids), batch_size_micro_for_no_grad):
        sequence_ids_batch = sequence_ids[i:i + batch_size_micro_for_no_grad]
        sequence_mask_batch = sequence_mask[i:i + batch_size_micro_for_no_grad]

        prob_old_batch = get_per_token_log_probs(
            model_policy,  # ä½¿ç”¨å½“å‰policyä½œä¸ºold policy
            input_ids=sequence_ids_batch,
            attention_mask=sequence_mask_batch,
        )
        prob_ref_batch = get_per_token_log_probs(
            model_reference,
            input_ids=sequence_ids_batch,
            attention_mask=sequence_mask_batch,
        )

        prob_per_token_old.append(prob_old_batch)
        prob_per_token_reference.append(prob_ref_batch)

    # å°†mini batchç»“æœæ‹¼æ¥
    prob_per_token_old = torch.cat(prob_per_token_old, dim=0)
    prob_per_token_reference = torch.cat(prob_per_token_reference, dim=0)

```

è®¡ç®—è¿™é‡Œç”¨äº† torch.no_grad()ï¼Œä½† rollout å¤ªå¤šäº†è¿˜æ˜¯ä¼šè¶…æ˜¾å­˜ï¼Œå› æ­¤è¿™é‡Œè¿˜æ˜¯é‡‡ç”¨ mini batch



#### grpo loss è®¡ç®—å¹¶åå‘ä¼ æ’­

è¿™é‡Œ FWD å’Œ BWD éƒ½é‡‡ç”¨äº† mini batch

```python
for _ in range(mu):

    optimizer.zero_grad()
    for i in range(0, len(sequence_ids), batch_size_micro):

        sequence_ids_batch = sequence_ids[i:i + batch_size_micro]
        sequence_mask_batch = sequence_mask[i:i + batch_size_micro]
        completion_mask_batch = completion_mask[i:i + batch_size_micro]
        group_advantage_per_sample_batch = group_advantage_per_sample[i:i + batch_size_micro]

        # ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„å›ºå®šold_policy_probå’Œreference_prob
        prob_per_token_old_batch = prob_per_token_old[i:i + batch_size_micro]
        prob_per_token_reference_batch = prob_per_token_reference[i:i + batch_size_micro]

        loss = get_grpo_loss(
            model_policy,
            sequence_ids_batch,
            sequence_mask_batch,
            completion_mask_batch,
            group_advantage_per_sample_batch,
            prob_per_token_old_batch,
            prob_per_token_reference_batch,
            epsilon,
            beta
        )
        loss.backward()
    optimizer.step()
```



## GRPO ç»“æœ

![format_reward](images/README/format_reward.png)



![tag_count_reward](images/README/tag_count_reward.png)

![accuracy_reward](images/README/accuracy_reward.png)

![mean_reward](images/README/mean_reward.png)

æ•´ä½“ reward æ›²çº¿ç®—æ­£å¸¸ã€‚tag å’Œ format å¾ˆå¿«å­¦ä¹ åˆ°ï¼Œaccuracy æ¯”è¾ƒéš¾å­¦ï¼Œä¸”æ³¢åŠ¨ä¹Ÿå¾ˆå¤§ã€‚





## GRPO trainer ç»“æœ

![image-20250802222140659](images/README/image-20250802222140659.png)

![image-20250802222237144](images/README/image-20250802222237144.png)

![image-20250802222156074](images/README/image-20250802222156074.png)

![image-20250802222248158](images/README/image-20250802222248158.png)

åŸºäº GRPO trainer çš„ç»“æœæ›´å¥½ç‚¹ï¼Œå…¶ xml reward ä¸Šé™ä¸º 0.5ï¼Œformat reward ä¸Šé™ä¸º 0.5ï¼Œcorrectness reward ä¸Šé™ä¸º 2ï¼Œæ‰€ä»¥å…¶reward ä¸Šé™ä¸º3(2~2.5ä¹‹é—´æ³¢åŠ¨ï¼Œå¹³å‡2.25)ã€‚æˆ‘è‡ªå·±çš„ä»£ç æ˜¯ä¸‰è€…çš„å¹³å‡å€¼ï¼Œæ‰€ä»¥ä¸Šé™ä¸º 1 (0.8~1ä¹‹é—´æ³¢åŠ¨ï¼Œå¹³å‡0.9)ã€‚

